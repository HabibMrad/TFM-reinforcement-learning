{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from collections import defaultdict\n",
    "\n",
    "# OpenAI gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba, *OpenAI Gym*, 2016, [arXiv:1606.01540](https://arxiv.org/abs/1606.01540), [GitHub:openai/gym](https://github.com/openai/gym).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF ENVIRONMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said an environment comprise everything outside the agent, so its code implementation has to contain all the functionality to allow the agent to interact with it and to learn. We are going to implement this environment according to the guidelines of OpenAI gym \\[1\\], one on the most used libraries to develop reinforcement learning applications.\n",
    "\n",
    "The environments are defined as a class with the following method:\n",
    "* __init(self)__: define initial information of the environment such as the observation space or the action space.\n",
    "* __reset(self)__: reset the environment's state.\n",
    "* __step(self, action)__: step the environment by one timestep. Returns\n",
    "    * _observation_ (object): an environment-specific object representing our observation of the environment. \n",
    "    * _reward_ (float): amount of reward achieved by the previous action.\n",
    "    * _done_ (boolean): whether it’s time to reset the environment again.\n",
    "    * _info_ (dict): diagnostic information useful for debugging. \n",
    "* __render(self, mode='human')__: render one frame of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is to define how an agent should perceive its environment, that is, we need to consider how a human will perform the task. In this sense, we can think about which kind of observations uses a human when trading:\n",
    "* Stock prices: the historical values of the price of a stock (open, hihg, low, close, volume) are the principal source of information to take decissions.\n",
    "* Protfolio status: Account balance, Stock positions, profit\n",
    "\n",
    "\n",
    "\n",
    "The intuition here is that for each time step, we want our agent to consider the price action leading up to the current price, as well as their own portfolio’s status in order to make an informed decision for the next action.\n",
    "Once a trader has perceived their environment, they need to take an action. In our agent’s case, its action_space will consist of three possibilities: buy a stock, sell a stock, or do nothing.\n",
    "But this isn’t enough; we need to know the amount of a given stock to buy or sell each time. Using gym’s Box space, we can create an action space that has a discrete number of action types (buy, sell, and hold), as well as a continuous spectrum of amounts to buy/sell (0-100% of the account balance/position size respectively).\n",
    "You’ll notice the amount is not necessary for the hold action, but will be provided anyway. Our agent does not initially know this, but over time should learn that the amount is extraneous for this action.\n",
    "The last thing to consider before implementing our environment is the reward. We want to incentivize profit that is sustained over long periods of time. At each step, we will set the reward to the account balance multiplied by some fraction of the number of time steps so far.\n",
    "The purpose of this is to delay rewarding the agent too fast in the early stages and allow it to explore sufficiently before optimizing a single strategy too deeply. It will also reward agents that maintain a higher balance for longer, rather than those who rapidly gain money using unsustainable strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            Adj Close     Close      High       Low      Open       Volume\n2000-01-03   3.470226  3.997768  4.017857  3.631696  3.745536  133949200.0\n2000-01-04   3.177650  3.660714  3.950893  3.613839  3.866071  128094400.0\n2000-01-05   3.224152  3.714286  3.948661  3.678571  3.705357  194580400.0\n2000-01-06   2.945139  3.392857  3.821429  3.392857  3.790179  191993200.0\n2000-01-07   3.084645  3.553571  3.607143  3.410714  3.446429  115183600.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Adj Close</th>\n      <th>Close</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Open</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2000-01-03</th>\n      <td>3.470226</td>\n      <td>3.997768</td>\n      <td>4.017857</td>\n      <td>3.631696</td>\n      <td>3.745536</td>\n      <td>133949200.0</td>\n    </tr>\n    <tr>\n      <th>2000-01-04</th>\n      <td>3.177650</td>\n      <td>3.660714</td>\n      <td>3.950893</td>\n      <td>3.613839</td>\n      <td>3.866071</td>\n      <td>128094400.0</td>\n    </tr>\n    <tr>\n      <th>2000-01-05</th>\n      <td>3.224152</td>\n      <td>3.714286</td>\n      <td>3.948661</td>\n      <td>3.678571</td>\n      <td>3.705357</td>\n      <td>194580400.0</td>\n    </tr>\n    <tr>\n      <th>2000-01-06</th>\n      <td>2.945139</td>\n      <td>3.392857</td>\n      <td>3.821429</td>\n      <td>3.392857</td>\n      <td>3.790179</td>\n      <td>191993200.0</td>\n    </tr>\n    <tr>\n      <th>2000-01-07</th>\n      <td>3.084645</td>\n      <td>3.553571</td>\n      <td>3.607143</td>\n      <td>3.410714</td>\n      <td>3.446429</td>\n      <td>115183600.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "stocks = pd.read_csv('data/AAPL_2000_2019.csv', index_col = 0, parse_dates = True)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleStockEnvironment:\n",
    "    '''\n",
    "    Reinforcement Leaning environment representing a Stock Market with a single stock.\n",
    "\n",
    "    Attributes:\n",
    "        data: series of prices\n",
    "        history: series of positions\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data['Close'])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "\n",
    "        # Create an empty history\n",
    "        self.history = np.zeros_like(self.data)\n",
    "        self.positions = []\n",
    "\n",
    "        # Create current values for these histories\n",
    "        self.history_t = 0\n",
    "        self.position = 0\n",
    "        return self.history\n",
    "\n",
    "    def step(self, action):\n",
    "        self.history_t = action\n",
    "        # Execute the action\n",
    "        if action == 0:\n",
    "            self.history[self.t] = self.history_t\n",
    "        elif action == 1:\n",
    "            self.history[self.t] = self.history_t\n",
    "        elif action == 2:\n",
    "            self.history[self.t] = self.history_t\n",
    "        \n",
    "        # Prepare the next step\n",
    "        self.t += 1\n",
    ""
   ]
  }
 ]
}